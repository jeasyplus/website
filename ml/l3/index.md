# 降低损失
如何减小误差？
先确定向参数空间的哪个方面前进。
沿着这个方向，我们可以知道所选的每组超参数都比以前的一组参数误差要小一些。
要获取具体方向，有一种方法是计算梯度，给予模型相关的误差函数的倒数。

获得数据后，我们计算这些数据的误差函数的梯度，我们通过负梯度得知在哪个方向更新模型才能减小误差，我们就朝着这个方向前进，获取新版模型。
然后重新计算梯度，然后重复这个过程。

![梯度下降法示意图](https://jeasyplus.com/images/machine-learning/2023-06-26-18.08.18.png)

模型参数与误差之间的关系如下面所示
![误差函数](https://jeasyplus.com/images/machine-learning/2023-06-26-18.25.44.png)

学习速率（超参数）
学习速率太大在多维度时会导致模型发生偏离。

## 权重初始化
+ 对于凸形问题，权重可以从任何位置开始（例如，所有值均为 0）
  + 凸形：画碗状
  + 只需一个最低要求
  + 前景：神经网络不成立
+ 非凸形：想象一个鸡蛋箱
  + 不止一个
  + 高度依赖于初始值

## SGD&小批量梯度下降法
+ 可以在每一步中计算整个数据集的梯度，但事实证明没有必要这样做
+ 计算小型数据样本的梯度效果良好
  + 每一步获取一个新的随机样本
+ 随机梯度下降法：一次抽取一个样本
+ 小批量梯度下降法：批次数量介于 10 到 1000 之间
  + 损失和梯度是针对批量计算得到的平均值

## 迭代方法

上一个模块引入了损失的概念。在本单元中，您将了解机器学习模型如何以迭代方式降低损失。

[迭代方法](https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach?hl=zh-cn "迭代方法")


## 梯度下降法

[梯度下降法](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent?hl=zh-cn "梯度下降法")


## 学习速率
[学习速率](https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate?hl=zh-cn "学习速率")

## 随机梯度下降法
在梯度下降法中，批次用于计算单次迭代中的梯度的样本总数。到目前为止，我们假设该批次已经是整个数据集。在 Google 规模下工作时，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含大量特征。因此，一批文件可能非常庞大。超大批量可能会导致一次迭代需要很长时间才能完成计算。

包含随机采样样本的大型数据集可能包含冗余数据。事实上，随着批次大小的增大，冗余也变得越来越多。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值通常不会比大批量高。

如果我们可以通过更少的计算量获得正确的平均梯度，会怎么样？通过从数据集中随机选择样本，我们可以估算（尽管比较嘈杂）某个样本的平均值较高。随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批次大小为 1）。考虑到迭代次数足够多，SGD 可以正常运行，但非常嘈杂。“随机”一词表示每批数据的一个示例是随机选择的。

**小批量随机梯度下降法（小批量 SGD）** 是全批量迭代与 SGD 之间的折衷方案。小批量通常是随机选择的 10 到 1000 个样本。小批量 SGD 可以减少 SGD 中的噪声量，但仍然比全批量更高效。

为了简化说明，我们重点介绍单个特征的梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。

[随机梯度下降法](https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent?hl=zh-cn "随机梯度下降法")


## 学习速率和收敛

在机器学习和深度学习中，超参数（Hyperparameters）是指那些在训练模型之前需要手动设置的参数，无法通过模型的训练过程自动学习得到。超参数的值会直接影响模型的学习过程和性能。

与超参数不同，模型的权重和偏差是在训练过程中通过优化算法自动学习得到的。而超参数需要我们在训练开始之前手动选择或调整。

## 如何减少损失？
+ 超参数是用于调整模型训练方式的配置设置。

+ (y - y')2 相对于权重和偏差的导数可让我们了解指定样本的损失如何变化

  + 计算和转化简单

+ 因此，我们反复朝着尽可能减少损失的方向迈出小步
  + 我们将这些小步称为梯度步长（但它们实际上是负梯度步长）
  + 这种策略称为梯度下降法

## 训练

![图](https://jeasyplus.com/images/machine-learning/2023-06-26-21.35.41.png)





